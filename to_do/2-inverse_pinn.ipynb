{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Problem: 1D Euler\n",
    "\n",
    "An **inverse problem** asks you to “turn the tables” on the usual forward simulation:\n",
    "\n",
    "> **“Given sparse or noisy observations of a system *and* the governing equations, infer unknown fields or parameters.”**\n",
    "\n",
    "We'll consider, for this second part, the 1D Euler setting. As such, in a 1D Euler inverse problem, that means:\n",
    "\n",
    "1. **Known physics**:\n",
    "  You still trust the Euler equations\n",
    "\n",
    "  $$\n",
    "    \\begin{aligned}\n",
    "      &\\partial_t \\rho + \\partial_x(\\rho\\,u) = 0,\\\\\n",
    "      &\\partial_t (\\rho u) + \\partial_x(\\rho u^2 + p) = 0,\\\\\n",
    "      &\\partial_t(\\rho E) + \\partial_x\\bigl(u(\\rho E + p)\\bigr) = 0,\n",
    "    \\end{aligned}\n",
    "  $$\n",
    "\n",
    "  with the closure\n",
    "\n",
    "  $$\n",
    "    p = (\\gamma-1)\\Bigl(\\rho E - \\tfrac12\\rho\\,u^2\\Bigr),\n",
    "    \\quad \\gamma=1.4.\n",
    "  $$\n",
    "\n",
    "2. **Available data**:\n",
    "  Instead of knowing the full solution $(\\rho,u,p)$ everywhere, you only “measure”:\n",
    "\n",
    "  * The *gradient* of density via Schlieren-like data (i.e.\\ $\\partial_x\\rho$) at many $(x,t)$ locations.\n",
    "  * A time‐series of *point* pressure measurements $p(x_p,t)$ at one or a few probe locations $x_p$.\n",
    "\n",
    "3. **Unknown targets**:\n",
    "  You wish to reconstruct the full fields\n",
    "\n",
    "  $$\n",
    "    \\rho(x,t),\\quad u(x,t),\\quad p(x,t),\n",
    "  $$\n",
    "\n",
    "  even though you’ve only directly “seen” parts of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Why inverse problems are (often) harder than forward ones\n",
    "\n",
    "* **Ill-posedness.**  Small noise in measurements can cause large swings in the inferred solution.  Unless you regularize or “anchor” the inference with strong physics, you risk non-uniqueness or instability.\n",
    "* **Data sparsity.**  Measuring full-field variables is expensive; you typically only get partial glimpses (e.g.\\ Schlieren gives you gradients, not absolute values, and probes give you one point in space).\n",
    "* **Nonlinearity & coupling.**  The Euler equations tightly couple $\\rho$, $u$, and $p$.  Errors in one inferred field feed into others via the PDE constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### PINNs for data-driven discovery\n",
    "\n",
    "Physics-Informed Neural Networks tackle this by training a network\n",
    "\n",
    "$$\n",
    "  (\\hat\\rho,\\hat u,\\hat p)(x,t;\\theta)\n",
    "$$\n",
    "\n",
    "to simultaneously:\n",
    "\n",
    "1. **Match measurements**\n",
    "\n",
    "  $$\n",
    "    \\bigl\\|\\partial_x\\hat\\rho(x_i,t_i) - \\text{measured }\\partial_x\\rho\\bigr\\|^2\n",
    "    +\n",
    "    \\bigl\\|\\hat p(x_p,t_j) - \\text{measured }p\\bigr\\|^2\n",
    "    \\;\\approx\\;0.\n",
    "  $$\n",
    "\n",
    "2. **Satisfy the PDE residuals**\n",
    "\n",
    "  $$\n",
    "    \\bigl\\|\\partial_t \\hat\\rho + \\partial_x(\\hat\\rho\\,\\hat u)\\bigr\\|^2\n",
    "    +\n",
    "    \\bigl\\|\\partial_t(\\hat\\rho\\,\\hat u) + \\partial_x(\\hat\\rho\\hat u^2 + \\hat p)\\bigr\\|^2\n",
    "    + \\dots\n",
    "    \\;\\approx\\;0.\n",
    "  $$\n",
    "\n",
    "3. **Honor initial/boundary conditions**\n",
    "\n",
    "  $$\n",
    "    \\hat\\rho(x,0)=1.0+0.2\\sin(\\pi x),\\quad \\hat u(x,0)=1.0,\\quad \\hat p(x,0)=1.0,\n",
    "  $$\n",
    "\n",
    "  and any boundary constraints on $x=\\pm1,\\ t\\in[0,2]$.\n",
    "\n",
    "During training, the network learns to reconcile the sparse, noisy data with the rigorous PDE constraints, effectively “discovering” fields that both explain the measurements and obey the laws of gas dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Forward vs. Inverse in our exercise\n",
    "\n",
    "* **Forward problem** (what you just tackled): $\\lambda$, PDE form, and initial/boundary data are known → solve for $u(x,t)$.\n",
    "* **Inverse problem** (this exercise): PDE form and γ are known, you have only gradient and probe measurements → infer $\\rho$, $u$, $p$ everywhere.\n",
    "\n",
    "We’ll follow the strategy from Appendix B of “Physics-informed neural networks for high-speed flows”: set up a PINN that ingests the Schlieren-like $\\partial_x\\rho$ and point-probe $p$ data, enforces the 1D Euler residuals, and recovers the full solution fields on $[-1,1]\\times[0,2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\"figure.figsize\": (6, 4), \"font.size\": 12})\n",
    "\n",
    "# Sets up GPU for PyTorch if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'use_tqdm': True,\n",
    "    'seed': 2,\n",
    "}\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerNet(nn.Module):\n",
    "    def __init__(self, layers=(2, *[120]*4, 3)):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for i in range(len(layers)-1):\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            if i < len(layers)-2:\n",
    "                modules.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_euler(\"../data/1DEuler_data.npy\")\n",
    "model = EulerNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3000\n",
    "lbfgs_start=2700\n",
    "lr=1e-3\n",
    "weight_decay=1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.5)\n",
    "\n",
    "losses = []\n",
    "metrics = {}\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        data[key] = value.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the physics‑informed loss\n",
    "\n",
    "In this section we will assemble the *physics* portion of our PINN loss, enforcing that the network outputs\n",
    "$(\\rho,u,p)$ satisfy the 1D Euler equations in **conservative form** with the conserved energy variable $\\rho E$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\partial_t \\rho + \\partial_x(\\rho\\,u) = 0,\\\\\n",
    "&\\partial_t(\\rho\\,u) + \\partial_x(\\rho\\,u^2 + p) = 0,\\\\\n",
    "&\\partial_t(\\rho E) + \\partial_x\\bigl(u\\,(\\rho E + p)\\bigr) = 0,\n",
    "\\end{aligned}\n",
    "\\qquad\n",
    "E = \\frac{p}{(\\gamma-1)} + \\frac12\\,\\rho\\,u^2.\n",
    "$$\n",
    "\n",
    "### Your tasks\n",
    "\n",
    "1. **Energy**\n",
    "   Compute the energy $E$ *before* automatic differentiation:\n",
    "\n",
    "   $$\n",
    "     E = \\frac{p}{(\\gamma - 1)} + \\frac12\\,\\rho^2\\,u^2.\n",
    "   $$\n",
    "\n",
    "2. **Calculate gradients**\n",
    "   Use autograd to compute the following derivatives:\n",
    "\n",
    "   $$\n",
    "     \\rho_t,\\ \\rho_x,\\ u_t,\\ u_x,\\ p_t,\\ p_x,\\ E_t,\\ E_x.\n",
    "   $$\n",
    "\n",
    "3. **Residuals**\n",
    "\n",
    "   * Mass:\n",
    "\n",
    "     $$\n",
    "     F_1 = \\rho_t + \\partial_x(\\rho\\,u)\n",
    "         = \\rho_t + u\\,\\rho_x + \\rho\\,u_x.\n",
    "     $$\n",
    "   * Momentum:\n",
    "\n",
    "     $$\n",
    "     F_2\n",
    "     = \\partial_t(\\rho\\,u) + \\partial_x(\\rho\\,u^2 + p)\\\\\n",
    "     = \\rho\\,u_t + u\\,\\rho_t + 2\\,\\rho\\,u\\,u_x + \\rho_x\\,u^2 + p_x.\n",
    "     $$\n",
    "   * Energy (conserved):\n",
    "\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     F_3\n",
    "     &= \\partial_t(\\rho E) + \\partial_x\\bigl(u(\\rho E + p)\\bigr) \\\\\n",
    "     &= (\\rho E)_t + u_x\\,(\\rho E + p) + u\\,\\bigl[(\\rho E)_x + p_x\\bigr] \\\\\n",
    "     &= (\\rho E)_t + u_x\\,\\rho E + u_x\\,p + u\\,(\\rho E)_x + u\\,p_x \\\\\n",
    "     &= \\rho_t\\,E \\;+\\;\\rho\\,E_t+u_x\\,\\rho E \\;+\\;u_x\\,p \\;+\\;u\\,\\rho_x\\,E \\;+\\;u\\,\\rho\\,E_x \\;+\\;u\\,p_x.\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "4. **Data losses**\n",
    "\n",
    "   * Probe pressure loss:\n",
    "\n",
    "     $$\n",
    "       \\mathcal L_{\\rm probe} = \\frac{1}{N}\\sum\\bigl(p_{\\rm probe}-p_{\\rm pred}\\bigr)^2.\n",
    "     $$\n",
    "\n",
    "5. **Final loss**\n",
    "\n",
    "   $$\n",
    "   \\mathcal L\n",
    "   = \\underbrace{\\langle F_1^2 + F_2^2 + F_3^2\\rangle}_{\\rm PDE}\n",
    "   + \\underbrace{\\mathcal L_{\\rm grad}}_{\\rm grad.\\;match}\n",
    "   + \\underbrace{\\mathcal L_{\\rm probe}}_{\\rm probe}\n",
    "   + \\underbrace{\\mathcal L_{\\rm mass}}_{\\rm mass\\;int.}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DX = 0.0999\n",
    "gamma = 1.4\n",
    "\n",
    "def physics_loss(F_pred, E_F, grads, rho_nr, rhodx_nr, grads_nr, p_probe, p_pred, rho_int0, rho_int_pred):\n",
    "    \"\"\"\n",
    "    Computes combined loss: PDE residuals, gradient-matching, probe pressure, and mass integral constraints.\n",
    "    \"\"\"\n",
    "    rho_F, u_F, p_F = F_pred[:, 0], F_pred[:, 1], F_pred[:, 2]\n",
    "    \n",
    "    # Derivatives\n",
    "    rho_g, u_g, p_g, E_g = grads\n",
    "    rhox_F, ux_F, px_F, Ex_F = None, None, None, None\n",
    "    rhot_F, ut_F, pt_F, Et_F = None, None, None, None\n",
    "\n",
    "    # PDE residuals\n",
    "    # TODO: Calculate each term of the PDE residuals\n",
    "    F1 = None\n",
    "    F2 = None\n",
    "    F3 = None\n",
    "    loss_F = (F1.pow(2).mean() + F2.pow(2).mean() + F3.pow(2).mean())\n",
    "\n",
    "    # gradient matching\n",
    "    rho_nr_NN, rhodx_nr_NN = grads_nr\n",
    "    grad_diff = ((rhodx_nr_NN - rho_nr_NN) / DX) - (rhodx_nr - rho_nr) / DX\n",
    "    loss_grad = grad_diff.pow(2).mean()\n",
    "\n",
    "    # probe pressure\n",
    "    # TODO: Calculate the probe pressure loss\n",
    "    loss_probe = None\n",
    "\n",
    "    # mass integral\n",
    "    loss_mass0 = (rho_int_pred - rho_int0).pow(2).mean()\n",
    "\n",
    "    return loss_F + loss_grad + loss_probe + loss_mass0, loss_F, loss_grad, loss_probe, loss_mass0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    F_pred = model(data['Phi_F'])\n",
    "\n",
    "    # TODO: Calculate the total energy E_F from the other variables\n",
    "    E     = None\n",
    "\n",
    "    # Compute gradients\n",
    "    # TODO: Calculate the gradients of F_pred and E with respect to data['Phi_F']\n",
    "    rho_g = None\n",
    "    u_g   = None\n",
    "    p_g   = None\n",
    "    E_g   = None\n",
    "    grads = (rho_g, u_g, p_g, E_g)\n",
    "    \n",
    "    # For other loss terms\n",
    "    rho_nr_NN = model(data['Phi_nabla'])[:, 0]\n",
    "    rhodx_nr_NN = model(data['Phi_nabla_dx'])[:, 0]\n",
    "    p_pred = model(data['Phi_probe'])[:, 2]\n",
    "    rho_pred_m0 = model(data['Phi_mass0'])[:, 0]\n",
    "    rho_int_pred = torch.trapz(rho_pred_m0, data['Phi_mass0'][:, 0])\n",
    "\n",
    "    loss, loss_F, loss_grad, loss_probe, loss_mass0 = physics_loss(\n",
    "        F_pred, E, grads,\n",
    "        data['rho_nr'], data['rhodx_nr'],\n",
    "        (rho_nr_NN, rhodx_nr_NN),\n",
    "        data['p_probe'], p_pred,\n",
    "        data['rho_int0'], rho_int_pred\n",
    "    )\n",
    "    loss.backward()\n",
    "    metrics['loss'] = loss.item()\n",
    "    metrics['loss_F'] = loss_F.item()\n",
    "    metrics['loss_grad'] = loss_grad.item()\n",
    "    metrics['loss_probe'] = loss_probe.item()\n",
    "    metrics['loss_mass0'] = loss_mass0.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = tqdm(range(1, epochs+1), disable=not config['use_tqdm'])\n",
    "for epoch in iterable:\n",
    "    if epoch == lbfgs_start:\n",
    "        print(\"Switching to LBFGS optimizer\")\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            model.parameters(),\n",
    "            max_iter=50,\n",
    "            tolerance_grad=1e-6\n",
    "        )\n",
    "    if epoch < lbfgs_start:\n",
    "        loss = closure()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        loss = optimizer.step(closure)\n",
    "\n",
    "    losses.append(\n",
    "        (metrics['loss'], metrics['loss_F'], metrics['loss_grad'],\n",
    "         metrics['loss_probe'], metrics['loss_mass0'])\n",
    "    )\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Total: {losses[-1][0]:.3e} \"\n",
    "              f\"| PDE: {losses[-1][1]:.3e} | Gradient Matching: {losses[-1][2]:.3e}\"\n",
    "              f\" | Probe: {losses[-1][3]:.3e} | Mass Integral: {losses[-1][4]:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = np.array(losses)\n",
    "plt.figure()\n",
    "plt.semilogy(loss_arr[:,0], label='Total')\n",
    "plt.semilogy(loss_arr[:,1], '--', label='PDE + Grad')\n",
    "plt.semilogy(loss_arr[:,2], '--', label='Gradient Matching')\n",
    "plt.semilogy(loss_arr[:,3], '--', label='Probe Pressure')\n",
    "plt.semilogy(loss_arr[:,4], '--', label='Mass Integral')\n",
    "plt.axvline(x=lbfgs_start, color='k', linestyle='--', label='LBFGS Start')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# adapt number of validation points\n",
    "val_points = 500\n",
    "\n",
    "# Create grid\n",
    "x = np.linspace(-1, 1, val_points)\n",
    "t = np.linspace(0, 2, val_points)\n",
    "X, T = np.meshgrid(x, t)\n",
    "Phi = torch.tensor(np.stack([X.flatten(), T.flatten()], axis=1), dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(Phi).cpu().numpy()\n",
    "rho_pred, u_pred, p_pred = output[:, 0], output[:, 1], output[:, 2]\n",
    "\n",
    "rho_true = 1.0 + 0.2*np.sin(np.pi*(X - T))\n",
    "u_true   = np.ones_like(rho_true)\n",
    "p_true   = np.ones_like(rho_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(18, 8))\n",
    "titles = ['rho (true)', 'u (true)', 'p (true)', 'rho (NN)', 'u (NN)', 'p (NN)']\n",
    "data_list = [rho_true, u_true, p_true, rho_pred, u_pred, p_pred]\n",
    "for i, data_i in enumerate(data_list):\n",
    "    im = ax[i//3, i%3].imshow(data_i.reshape(val_points, val_points), aspect='auto', vmin=0, vmax=1.2)\n",
    "    ax[i//3, i%3].set_title(titles[i])\n",
    "    fig.colorbar(im, ax=ax[i//3, i%3])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, val_points)\n",
    "t = np.linspace(0,    2, val_points)\n",
    "rho_true_mesh = rho_true.reshape(val_points, val_points)\n",
    "u_true_mesh   = u_true.reshape(val_points, val_points)\n",
    "p_true_mesh   = p_true.reshape(val_points, val_points)\n",
    "rho_pred_mesh = rho_pred.reshape(val_points, val_points)\n",
    "u_pred_mesh   = u_pred.reshape(val_points, val_points)\n",
    "p_pred_mesh   = p_pred.reshape(val_points, val_points)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 9), sharex=True)\n",
    "lines = []\n",
    "\n",
    "for ax, true_m, pred_m, name in zip(\n",
    "    axes,\n",
    "    (rho_true_mesh, u_true_mesh, p_true_mesh),\n",
    "    (rho_pred_mesh, u_pred_mesh, p_pred_mesh),\n",
    "    (\"rho\", \"u\", \"p\")\n",
    "):\n",
    "    lt, = ax.plot([], [], \"b-\",  label=f\"{name} true\")\n",
    "    lp, = ax.plot([], [], \"r--\", label=f\"{name} NN\")\n",
    "    lines.append((lt, lp))\n",
    "\n",
    "    ymin, ymax = true_m.min(), true_m.max()\n",
    "    ymin = ymin - 0.01*(ymax-ymin)\n",
    "    ymax = ymax + 0.01*(ymax-ymin)\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    ax.set_ylabel(name)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "axes[-1].set_xlabel(\"x\")\n",
    "fig.tight_layout()\n",
    "\n",
    "def init():\n",
    "    for lt, lp in lines:\n",
    "        lt.set_data([], [])\n",
    "        lp.set_data([], [])\n",
    "    return [l for pair in lines for l in pair]\n",
    "\n",
    "def update(frame):\n",
    "    for (lt, lp), true_m, pred_m in zip(\n",
    "        lines,\n",
    "        (rho_true_mesh, u_true_mesh, p_true_mesh),\n",
    "        (rho_pred_mesh, u_pred_mesh, p_pred_mesh)\n",
    "    ):\n",
    "        lt.set_data(x, true_m[frame, :])\n",
    "        lp.set_data(x, pred_m[frame, :])\n",
    "    axes[0].set_title(f\"t = {t[frame]:.3f}\")\n",
    "    return [l for pair in lines for l in pair]\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig, update,\n",
    "    frames=val_points,\n",
    "    init_func=init,\n",
    "    blit=True,\n",
    "    interval=50\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn-workshop-aptwind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
