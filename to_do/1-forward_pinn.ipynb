{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks\n",
    "\n",
    "In this workshop, you will implement physics-informed neural networks (PINNs) and apply it to two different problems: a forward and a inverse one.\n",
    "\n",
    "For both of them, most of the code is already written - the goal is for you to take away the most important concepts. As shown in the lecture, the physics-informed component is to be encoded via the loss function, and that will be the emphasis of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\"figure.figsize\": (6, 4), \"font.size\": 12})\n",
    "\n",
    "# Sets up GPU for PyTorch if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'use_tqdm': True,\n",
    "    'seed': 2,\n",
    "}\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Forward Problem: Burger's Equation\n",
    "\n",
    "PINNs were first proposed in [Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations](https://arxiv.org/abs/1711.10561) and [Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations](https://arxiv.org/abs/1711.10566).\n",
    "\n",
    "These papers suggest using neural networks to model phenomena in physics and give an example with a PINN for the Burger's equation, since this equation is simple to understand, but can be tricky to solve numerically. In the first part of this workshop we are going to solve a forward problem (data-driven solution) with a PINN for the Burger's equation.\n",
    "\n",
    "A **forward problem** in the context of differential equations is simply this:\n",
    "\n",
    "> **“Given a governing equation together with its initial and boundary conditions, predict the solution field.”**\n",
    "\n",
    "In other words, we already know\n",
    "\n",
    "1. **The physics**—the partial differential equation (PDE) itself (here, the viscous Burgers’ equation),\n",
    "2. **The parameters**—for Burgers’ equation, the viscosity $\\lambda$ is known,\n",
    "3. **The constraints**—initial data $u(x,0)$ and boundary values on $x=-1$ and $x=1$.\n",
    "\n",
    "#### Forward vs. Inverse\n",
    "\n",
    "It's important not to conflate the **forward problem** (solve for $u$ when $\\lambda$ and the PDE form are known) with the **inverse problem** (infer unknown parameters—like $\\lambda$ or even the form of the nonlinearity—given measurements of $u$). PINNs tackle both, but in this first part we focus strictly on the **forward problem**, i.e., *data-driven solution* rather than *data-driven discovery*.\n",
    "\n",
    "#### Your Task\n",
    "\n",
    "Your task is to train a PINN $u_\\theta(x,t)$ to approximate the solution on $(x,t)\\in[-1,1]\\times[0,1]$ by minimizing two losses over:\n",
    "\n",
    "1. **IC/BC points** $\\Phi_{\\rm icbc} = \\{(x_i,t_i)\\}$:\n",
    "   Enforce\n",
    "\n",
    "   $$\n",
    "     u_\\theta(x_i,t_i) = u_{\\rm icbc}(x_i,t_i),\n",
    "     \\quad\n",
    "     u_{\\rm icbc}(x,0) = -\\sin(\\pi x),\\;\n",
    "     u_{\\rm icbc}(\\pm1,t)=0.\n",
    "   $$\n",
    "\n",
    "2. **Collocation points** $\\Phi_f = \\{(x_j,t_j)\\}$:\n",
    "   Enforce the PDE residual\n",
    "\n",
    "   $$\n",
    "     r(x_j,t_j)\n",
    "     = u_t + u\\,u_x - \\lambda\\,u_{xx}\n",
    "     = 0\n",
    "   $$\n",
    "\n",
    "In short, find $\\theta$ such that\n",
    "\n",
    "$$\n",
    "\\mathcal L \n",
    "= \\underbrace{\\frac1{|\\Phi_{\\rm icbc}|}\\sum_{\\Phi_{\\rm icbc}}\n",
    "\\bigl(u_\\theta - u_{\\rm icbc}\\bigr)^2}_{\\text{IC/BC loss}}\n",
    "+ \\underbrace{\\frac1{|\\Phi_f|}\\sum_{\\Phi_f}r^2}_{\\text{PDE residual loss}}\n",
    "$$\n",
    "\n",
    "is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6367,  0.6217],\n",
       "        [-0.2016,  0.5471],\n",
       "        [ 0.0759,  0.8639],\n",
       "        ...,\n",
       "        [ 0.0782,  0.2125],\n",
       "        [-0.0916,  0.5380],\n",
       "        [ 0.7190,  0.5947]], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/burgers_shock.mat'  # adjust path as needed\n",
    "icbc_points = 100\n",
    "f_points = 10000\n",
    "\n",
    "x, t, X_mesh, T_mesh, u_target_mesh, Phi_icbc, u_icbc, Phi_f = load_data_burgers(\n",
    "    path, icbc_points, f_points\n",
    ")\n",
    "\n",
    "# move data to GPU if available\n",
    "Phi_icbc, u_icbc, Phi_f = [v.to(device) for v in (Phi_icbc, u_icbc, Phi_f)]\n",
    "Phi_f.requires_grad_(True) # this is key for the loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurgersNet(nn.Module):\n",
    "    def __init__(self, layers=(2, *[20]*8, 1)):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for i in range(len(layers)-1):\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            if i < len(layers)-2:\n",
    "                modules.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = BurgersNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please note the loss terms:\n",
    "\n",
    "1. **Data loss** on initial and boundary conditions (IC/BC):  \n",
    "   $$\n",
    "   \\mathrm{MSE}_{\\mathrm{IC/BC}} = \\frac{1}{N_{\\mathrm{ic}}}\\sum_{i=1}^{N_{\\mathrm{ic}}} \\bigl(u_{\\mathrm{pred}}(t_i,x_i) - u_{\\mathrm{true}}(t_i,x_i)\\bigr)^2\n",
    "   $$\n",
    "\n",
    "2. **PDE residual loss** at collocation points:  \n",
    "   $$\n",
    "   \\mathrm{MSE}_{\\mathrm{PDE}} = \\frac{1}{N_{\\mathrm{col}}}\\sum_{j=1}^{N_{\\mathrm{col}}}\n",
    "   \\bigl(u_t + u\\,u_x - \\nu\\,u_{xx}\\bigr)^2\n",
    "   $$\n",
    "\n",
    "The total loss is\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{MSE}_{\\mathrm{IC/BC}} + \\mathrm{MSE}_{\\mathrm{PDE}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_loss(u_icbc_pred, u_icbc_true, u_f_pred, grads, viscosity=0.01/np.pi):\n",
    "    # TODO: implement the loss function\n",
    "    # you should return the total loss, but also the individual components\n",
    "    ut, ux, uxx = grads\n",
    "    mse_icbc = torch.mean((u_icbc_pred - u_icbc_true)**2)\n",
    "    mse_pde = torch.mean((ut + u_f_pred * ux - viscosity * uxx)**2)\n",
    "    total_loss = mse_icbc + mse_pde\n",
    "    return total_loss, mse_icbc.item(), mse_pde.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "lr = 2e-3\n",
    "scheduler_step = 2000\n",
    "scheduler_gamma = 0.5\n",
    "lbfgs_start = 1700\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "loss_hist = []\n",
    "metrics = {}\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # TODO: implement the autograd calls\n",
    "    # The model calculates the output for the initial/boundary conditions\n",
    "    # and also for the collocation points\n",
    "    # You should also calculate the gradients of the collocation points\n",
    "    # to use them in the loss function\n",
    "\n",
    "    # IC/BC prediction\n",
    "    u_icbc_pred = model(Phi_icbc)\n",
    "    # Collocation prediction\n",
    "    u_f_pred = model(Phi_f)\n",
    "    gradients = grad(u_f_pred.sum(), Phi_f, create_graph=True)[0]\n",
    "    ux = gradients[:, 0:1]\n",
    "    ut = gradients[:, 1:2]\n",
    "    uxx = grad(ux.sum(), Phi_f, create_graph=True)[0][:, 1:2]\n",
    "    loss_val, l_icbc, l_pde = pinn_loss(u_icbc_pred, u_icbc, u_f_pred, (ut, ux, uxx))\n",
    "    metrics['loss_val'] = loss_val\n",
    "    metrics['l_icbc'] = l_icbc\n",
    "    metrics['l_pde'] = l_pde\n",
    "    loss_val.backward()\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c70b64ddafc48e0b40d1983b71a7c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100 | Total: 1.477e-01 | ICBC: 1.345e-01 | PDE: 1.325e-02\n",
      "Epoch  200 | Total: 9.244e-02 | ICBC: 6.530e-02 | PDE: 2.715e-02\n",
      "Epoch  300 | Total: 7.050e-02 | ICBC: 5.075e-02 | PDE: 1.974e-02\n",
      "Epoch  400 | Total: 6.114e-02 | ICBC: 4.450e-02 | PDE: 1.663e-02\n",
      "Epoch  500 | Total: 5.683e-02 | ICBC: 4.139e-02 | PDE: 1.543e-02\n",
      "Epoch  600 | Total: 5.364e-02 | ICBC: 3.978e-02 | PDE: 1.386e-02\n",
      "Epoch  700 | Total: 5.384e-02 | ICBC: 3.817e-02 | PDE: 1.567e-02\n",
      "Epoch  800 | Total: 5.010e-02 | ICBC: 3.826e-02 | PDE: 1.184e-02\n",
      "Epoch  900 | Total: 4.855e-02 | ICBC: 3.748e-02 | PDE: 1.107e-02\n",
      "Epoch 1000 | Total: 4.764e-02 | ICBC: 3.707e-02 | PDE: 1.057e-02\n",
      "Epoch 1100 | Total: 4.832e-02 | ICBC: 3.679e-02 | PDE: 1.153e-02\n",
      "Epoch 1200 | Total: 4.738e-02 | ICBC: 3.791e-02 | PDE: 9.470e-03\n",
      "Epoch 1300 | Total: 4.825e-02 | ICBC: 3.794e-02 | PDE: 1.031e-02\n",
      "Epoch 1400 | Total: 6.996e-02 | ICBC: 5.434e-02 | PDE: 1.561e-02\n",
      "Epoch 1500 | Total: 5.769e-02 | ICBC: 4.331e-02 | PDE: 1.438e-02\n",
      "Epoch 1600 | Total: 5.431e-02 | ICBC: 4.104e-02 | PDE: 1.327e-02\n",
      "Switching to LBFGS optimizer\n",
      "Epoch 1700 | Total: 5.157e-02 | ICBC: 3.980e-02 | PDE: 1.177e-02\n",
      "Epoch 1800 | Total: nan | ICBC: nan | PDE: nan\n",
      "Epoch 1900 | Total: nan | ICBC: nan | PDE: nan\n",
      "Epoch 2000 | Total: nan | ICBC: nan | PDE: nan\n"
     ]
    }
   ],
   "source": [
    "iterable = tqdm(range(1, epochs+1), disable=not config['use_tqdm'])\n",
    "for epoch in iterable:\n",
    "    if epoch == lbfgs_start:\n",
    "        print(\"Switching to LBFGS optimizer\")\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            model.parameters(),\n",
    "            tolerance_grad=1e-6,\n",
    "        )\n",
    "    if epoch < lbfgs_start:\n",
    "        loss = closure()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        loss = optimizer.step(closure)\n",
    "\n",
    "    loss_hist.append((metrics['loss_val'].item(), metrics['l_icbc'], metrics['l_pde']))\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Total: {loss_hist[-1][0]:.3e} \"\n",
    "              f\"| ICBC: {loss_hist[-1][1]:.3e} | PDE: {loss_hist[-1][2]:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = np.array(loss_hist)\n",
    "plt.figure()\n",
    "plt.semilogy(loss_arr[:,0], label='Total')\n",
    "plt.semilogy(loss_arr[:,1], '--', label='ICBC')\n",
    "plt.semilogy(loss_arr[:,2], '--', label='PDE')\n",
    "plt.axvline(x=lbfgs_start, color='k', linestyle='--', label='LBFGS Start')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Phi_all = torch.from_numpy(\n",
    "        np.hstack([X_mesh.reshape(-1,1), T_mesh.reshape(-1,1)])\n",
    "    ).float().to(device)\n",
    "    u_pred = model(Phi_all).cpu().numpy().reshape(u_target_mesh.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "# True solution\n",
    "cf0 = axes[0].contourf(T_mesh, X_mesh, u_target_mesh, cmap='viridis')\n",
    "axes[0].set_title('True Solution')\n",
    "axes[0].set_xlabel('t')\n",
    "axes[0].set_ylabel('x')\n",
    "fig.colorbar(cf0, ax=axes[0])\n",
    "# PINN prediction\n",
    "cf1 = axes[1].contourf(T_mesh, X_mesh, u_pred, cmap='viridis')\n",
    "axes[1].set_title('PINN Prediction')\n",
    "axes[1].set_xlabel('t')\n",
    "axes[1].set_ylabel('x')\n",
    "fig.colorbar(cf1, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "line_true, = ax.plot([], [], 'b-', label='True')\n",
    "line_pred, = ax.plot([], [], 'r--', label='NN pred')\n",
    "ax.set_xlim(float(x.min()), float(x.max()))\n",
    "ymin = min(u_target_mesh.min(), u_pred.min())\n",
    "ymax = max(u_target_mesh.max(), u_pred.max())\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('u')\n",
    "ax.legend()\n",
    "\n",
    "def init():\n",
    "    line_true.set_data([], [])\n",
    "    line_pred.set_data([], [])\n",
    "    return line_true, line_pred\n",
    "\n",
    "def update(frame):\n",
    "    xx = x.flatten()\n",
    "    y_true = u_target_mesh[frame, :]\n",
    "    y_pred = u_pred[frame, :]\n",
    "    line_true.set_data(xx, y_true)\n",
    "    line_pred.set_data(xx, y_pred)\n",
    "    ax.set_title(f't = {float(t[frame,0]):.3f}')\n",
    "    return line_true, line_pred\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=u_pred.shape[0],\n",
    "                    init_func=init, blit=True, interval=100)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_l2 = np.linalg.norm((u_pred - u_target_mesh).ravel())/np.linalg.norm(u_target_mesh.ravel())\n",
    "print(f\"Relative L2 Error: {error_l2:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
